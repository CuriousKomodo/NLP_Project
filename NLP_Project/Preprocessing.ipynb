{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_exs(dataset):\n",
    "    \"\"\"\n",
    "    Returns the total number of (context, question, answer) triples,\n",
    "    given the data read from the SQuAD json file.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for article in dataset['data']:\n",
    "        for para in article['paragraphs']:\n",
    "            total += len(para['qas'])\n",
    "    return total\n",
    "\n",
    "\n",
    "def data_from_json(filename):\n",
    "    \"\"\"Loads JSON data from filename and returns\"\"\"\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenize(sequence):\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.word_tokenize(sequence)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_char_word_loc_mapping(context, context_tokens):\n",
    "    \"\"\"\n",
    "    Return a mapping that maps from character locations to the corresponding token locations.\n",
    "    If we're unable to complete the mapping e.g. because of special characters, we return None.\n",
    "\n",
    "    Inputs:\n",
    "      context: string (unicode)\n",
    "      context_tokens: list of strings (unicode)\n",
    "\n",
    "    Returns:\n",
    "      mapping: dictionary from ints (character locations) to (token, token_idx) pairs\n",
    "        Only ints corresponding to non-space character locations are in the keys\n",
    "        e.g. if context = \"hello world\" and context_tokens = [\"hello\", \"world\"] then\n",
    "        0,1,2,3,4 are mapped to (\"hello\", 0) and 6,7,8,9,10 are mapped to (\"world\", 1)\n",
    "    \"\"\"\n",
    "    acc = '' # accumulator\n",
    "    current_token_idx = 0 # current word loc\n",
    "    mapping = dict()\n",
    "\n",
    "    for char_idx, char in enumerate(context): # step through original characters\n",
    "        if char != u' ' and char != u'\\n': # if it's not a space:\n",
    "            acc += char # add to accumulator\n",
    "            context_token = str(context_tokens[current_token_idx]) # current word token\n",
    "            if acc == context_token: # if the accumulator now matches the current word token\n",
    "                syn_start = char_idx - len(acc) + 1 # char loc of the start of this word\n",
    "                for char_loc in range(syn_start, char_idx+1):\n",
    "                    mapping[char_loc] = (acc, current_token_idx) # add to mapping\n",
    "                acc = '' # reset accumulator\n",
    "                current_token_idx += 1\n",
    "\n",
    "    if current_token_idx != len(context_tokens):\n",
    "        return None\n",
    "    else:\n",
    "        return mapping\n",
    "\n",
    "def write_to_file(out_file, line):\n",
    "    #out_file.write(line.encode('utf8') + '\\n'.encode('utf8'))\n",
    "    out_file.write(str(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_and_write(dataset, tier, out_dir):\n",
    "    \"\"\"Reads the dataset, extracts context, question, answer, tokenizes them,\n",
    "    and calculates answer span in terms of token indices.\n",
    "    Note: due to tokenization issues, and the fact that the original answer\n",
    "    spans are given in terms of characters, some examples are discarded because\n",
    "    we cannot get a clean span in terms of tokens.\n",
    "\n",
    "    This function produces the {train/dev}.{context/question/answer/span} files.\n",
    "\n",
    "    Inputs:\n",
    "      dataset: read from JSON\n",
    "      tier: string (\"train\" or \"dev\")\n",
    "      out_dir: directory to write the preprocessed files\n",
    "    Returns:\n",
    "      the number of (context, question, answer) triples written to file by the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    num_exs = 0 # number of examples written to file\n",
    "    num_mappingprob, num_tokenprob, num_spanalignprob, num_empty_charloc = 0, 0, 0,0\n",
    "    examples = []\n",
    "\n",
    "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
    "        print('id=',articles_id)\n",
    "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "        for pid in range(len(article_paragraphs)):\n",
    "            context = str(article_paragraphs[pid]['context']) # string\n",
    "\n",
    "            # The following replacements are suggested in the paper\n",
    "            # BidAF (Seo et al., 2016)\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "\n",
    "            context_tokens = tokenize(context) # list of strings (lowercase)\n",
    "            context = context.lower()\n",
    "            \n",
    "            qas = article_paragraphs[pid]['qas']\n",
    "                \n",
    "            article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "\n",
    "            charloc2wordloc = get_char_word_loc_mapping(context, context_tokens) # charloc2wordloc maps the character location (int) of a context token to a pair giving (word (string), word loc (int)) of that token\n",
    "            '''\n",
    "            if charloc2wordloc is None: # there was a problem\n",
    "                num_mappingprob += len(qas)\n",
    "                continue # skip this context example\n",
    "            '''\n",
    "            # for each question, process the question and answer and write to file\n",
    "            for qn in qas:\n",
    "                \n",
    "                question = str(qn['question']) # string\n",
    "                question_tokens = tokenize(question) # list of strings\n",
    "\n",
    "                # of the three answers, just take the first\n",
    "                \n",
    "                if qn['is_impossible']==False:\n",
    "                    impossible = 0\n",
    "                    artifical_answer = 0\n",
    "                    ans_text = str(qn['answers'][0]['text']).lower() \n",
    "                    ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
    "                    ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
    "\n",
    "                    # Check that the provided character spans match the provided answer text\n",
    "\n",
    "                    if str(context[ans_start_charloc:ans_end_charloc]) != str(ans_text):\n",
    "                        # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
    "                        # We should upgrade to Python 3 next year!\n",
    "                        num_spanalignprob += 1      \n",
    "                        continue\n",
    "\n",
    "                    # get word locs for answer start and end (inclusive)\n",
    "                    #if (articles_id==14):\n",
    "                        #print('pid=',pid)\n",
    "                        #print(charloc2wordloc)\n",
    "                    \n",
    "                    if charloc2wordloc is None:\n",
    "                        num_empty_charloc+=1\n",
    "                        continue\n",
    "                        \n",
    "                    ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "                    ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "                    assert ans_start_wordloc <= ans_end_wordloc\n",
    "\n",
    "                    # Check retrieved answer tokens match the provided answer text.\n",
    "                    # Sometimes they won't match, e.g. if the context contains the phrase \"fifth-generation\"\n",
    "                    # and the answer character span is around \"generation\",\n",
    "                    # but the tokenizer regards \"fifth-generation\" as a single token.\n",
    "                    # Then ans_tokens has \"fifth-generation\" but the ans_text is \"generation\", which doesn't match.\n",
    "                    ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
    "                    \n",
    "                    \n",
    "                    if str(context[ans_start_charloc:ans_end_charloc]) != str(ans_text):\n",
    "                        # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 \n",
    "                        #https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
    "                        # We should upgrade to Python 3 next year!\n",
    "                        num_spanalignprob += 1      \n",
    "                        continue\n",
    "                        \n",
    "                    ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
    "                    \n",
    "                    \n",
    "                    # get word locs for answer start and end (inclusive)\n",
    "                    ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "                    ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "                    assert ans_start_wordloc <= ans_end_wordloc\n",
    "                    \n",
    "                    \n",
    "                    if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
    "                        num_tokenprob += 1\n",
    "                        continue # skip this question/answer pair if \n",
    "                    \n",
    "                    \n",
    "                else: \n",
    "                    impossible = 1\n",
    "                    #print(qn['plausible_answers'])\n",
    "                    if len(qn['plausible_answers'])>0:\n",
    "                        artifical_answer = 1\n",
    "                        ans_text = str(qn['plausible_answers'][0]['text']).lower()    \n",
    "                        ans_start_charloc = qn['plausible_answers'][0]['answer_start'] # answer start loc (character count)\n",
    "                    \n",
    "                    else: #empty implausible questions, meaning questions with no artificial answers. \n",
    "                        artifical_answer = 0\n",
    "                        ans_text = 'N/A'\n",
    "                        ans_start_charloc = 'N/A'\n",
    "                        ans_end_charloc = 'N/A'\n",
    "                        ans_start_wordloc = 0.5\n",
    "                        ans_end_wordloc = 0.5\n",
    "                        num_mappingprob+=1\n",
    "\n",
    "                \n",
    "                \n",
    "                examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)]),' '.join(str(impossible)),' '.join(str(artifical_answer ))))\n",
    "\n",
    "                num_exs += 1\n",
    "\n",
    "    print(\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
    "    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
    "    print(\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n",
    "    print(\"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
    "    \n",
    "\n",
    "    indices = list(range(len(examples)))\n",
    "    #np.random.shuffle(indices)\n",
    "\n",
    "    with open(os.path.join(out_dir, tier +'.context'), 'w') as context_file,\\\n",
    "    open(os.path.join(out_dir, tier +'.question'), 'w') as question_file,\\\n",
    "    open(os.path.join(out_dir, tier +'.answer'), 'w') as ans_text_file, \\\n",
    "    open(os.path.join(out_dir, tier +'.span'), 'w') as span_file,\\\n",
    "    open(os.path.join(out_dir, tier +'.impossible'), 'w') as impossible_boolean_file,\\\n",
    "    open(os.path.join(out_dir, tier +'.artificial_answer'), 'w') as artificial_boolean_file:\n",
    "\n",
    "        for i in indices:\n",
    "            (context, question, answer, answer_span, impossible, artifical_answer) = examples[i]\n",
    "\n",
    "            # write tokenized data to file\n",
    "            write_to_file(context_file, context)\n",
    "            write_to_file(question_file, question)\n",
    "            write_to_file(ans_text_file, answer)\n",
    "            write_to_file(span_file, answer_span)\n",
    "            write_to_file(impossible_boolean_file, impossible)\n",
    "            write_to_file(artificial_boolean_file, artifical_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Now apply the Preprocessing onto Squad 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data has 130319 examples total\n",
      "Dev data has 11873 examples total\n",
      "id= 0\n",
      "id= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:   9%|▊         | 3/35 [00:00<00:04,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 2\n",
      "id= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:  14%|█▍        | 5/35 [00:00<00:03,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 4\n",
      "id= 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  20%|██        | 7/35 [00:00<00:04,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 6\n",
      "id= 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  26%|██▌       | 9/35 [00:01<00:03,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 8\n",
      "id= 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  31%|███▏      | 11/35 [00:01<00:04,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 10\n",
      "id= 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  37%|███▋      | 13/35 [00:01<00:03,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 12\n",
      "id= 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:  43%|████▎     | 15/35 [00:02<00:02,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 14\n",
      "id= 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  49%|████▊     | 17/35 [00:02<00:02,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 16\n",
      "id= 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  57%|█████▋    | 20/35 [00:02<00:02,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 18\n",
      "id= 19\n",
      "id= 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  63%|██████▎   | 22/35 [00:02<00:01,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 21\n",
      "id= 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  69%|██████▊   | 24/35 [00:03<00:01,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 23\n",
      "id= 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:  71%|███████▏  | 25/35 [00:03<00:01,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:  74%|███████▍  | 26/35 [00:03<00:02,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:  77%|███████▋  | 27/35 [00:04<00:02,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:  80%|████████  | 28/35 [00:04<00:02,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  86%|████████▌ | 30/35 [00:05<00:01,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 29\n",
      "id= 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:  89%|████████▊ | 31/35 [00:05<00:01,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing dev:  91%|█████████▏| 32/35 [00:05<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:  97%|█████████▋| 34/35 [00:06<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id= 33\n",
      "id= 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev: 100%|██████████| 35/35 [00:06<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  15\n",
      "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  74\n",
      "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  0\n",
      "Processed 11799 examples of total 11888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"train-v2.0.json\"\n",
    "dev_filename = \"dev-v2.0.json\"\n",
    "data_dir = '/Users/kefei/Documents/DSML/NLP/Project'\n",
    "\n",
    "train_data = data_from_json(train_filename)\n",
    "print(\"Train data has %i examples total\" % total_exs(train_data))\n",
    "#preprocess_and_write(train_data, 'train', data_dir)\n",
    "\n",
    "dev_data = data_from_json(dev_filename)\n",
    "print(\"Dev data has %i examples total\" % total_exs(dev_data))\n",
    "\n",
    "# preprocess dev set and write to file\n",
    "preprocess_and_write(dev_data, 'dev', data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Codes used to debug!\n",
    "\n",
    "\n",
    "dataset = dev_data\n",
    "articles_id=7\n",
    "article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "pid = 14\n",
    "\n",
    "context = str(article_paragraphs[pid]['context']) # string\n",
    "\n",
    "# The following replacements are suggested in the paper\n",
    "# BidAF (Seo et al., 2016)\n",
    "context = context.replace(\"''\", '\" ')\n",
    "context = context.replace(\"``\", '\" ')\n",
    "\n",
    "context_tokens = tokenize(context) # list of strings (lowercase)\n",
    "context = context.lower()\n",
    "\n",
    "qas = article_paragraphs[pid]['qas']\n",
    "qn = qas[0]\n",
    "question = str(qn['question']) # string\n",
    "question_tokens = tokenize(question) # list of strings\n",
    "ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
    "\n",
    "ans_text = str(qn['answers'][0]['text']).lower()\n",
    "ans_end_charloc = ans_start_charloc + len(ans_text) \n",
    "\n",
    "question = str(qn['question'])\n",
    "\n",
    "charloc2wordloc = get_char_word_loc_mapping(context, context_tokens)\n",
    "ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
