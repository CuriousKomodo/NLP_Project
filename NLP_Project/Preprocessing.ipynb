{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SQuAD Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script organizes the SQuAD 2.0 training and dev dataset into the following attributes:\n",
    "\n",
    "    -context: string, relevant paragraph for each QA pair\n",
    "\n",
    "    -question: string, question\n",
    "\n",
    "    -answer: string, correct/plausible answer\n",
    "\n",
    "    -answer_span: size 2 array, indicate locations of start/ending of answer in the context\n",
    "\n",
    "    -impossible: boolean, 0/1, indicates whether or not the question is answerable \n",
    "\n",
    "    -artifical_answer: boolean, 0/1, only valid if 'impossible'==1, indicates whether or not the answer is correct or adverserial\n",
    "\n",
    "They are all saved in the format of train.context, trian.questions etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kefei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_exs(dataset):\n",
    "    \"\"\"\n",
    "    Returns the total number of (context, question, answer) triples,\n",
    "    given the data read from the SQuAD json file.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for article in dataset['data']:\n",
    "        for para in article['paragraphs']:\n",
    "            total += len(para['qas'])\n",
    "    return total\n",
    "\n",
    "\n",
    "def data_from_json(filename):\n",
    "    \"\"\"Loads JSON data from filename and returns\"\"\"\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def tokenize(sequence):\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.word_tokenize(sequence)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_char_word_loc_mapping(context, context_tokens):\n",
    "    \"\"\"\n",
    "    Return a mapping that maps from character locations to the corresponding token locations.\n",
    "    If we're unable to complete the mapping e.g. because of special characters, we return None.\n",
    "\n",
    "    Inputs:\n",
    "      context: string (unicode)\n",
    "      context_tokens: list of strings (unicode)\n",
    "\n",
    "    Returns:\n",
    "      mapping: dictionary from ints (character locations) to (token, token_idx) pairs\n",
    "        Only ints corresponding to non-space character locations are in the keys\n",
    "        e.g. if context = \"hello world\" and context_tokens = [\"hello\", \"world\"] then\n",
    "        0,1,2,3,4 are mapped to (\"hello\", 0) and 6,7,8,9,10 are mapped to (\"world\", 1)\n",
    "    \"\"\"\n",
    "    acc = '' # accumulator\n",
    "    current_token_idx = 0 # current word loc\n",
    "    mapping = dict()\n",
    "\n",
    "    for char_idx, char in enumerate(context): # step through original characters\n",
    "        if char != u' ' and char != u'\\n': # if it's not a space:\n",
    "            acc += char # add to accumulator\n",
    "            context_token = str(context_tokens[current_token_idx]) # current word token\n",
    "            if acc == context_token: # if the accumulator now matches the current word token\n",
    "                syn_start = char_idx - len(acc) + 1 # char loc of the start of this word\n",
    "                for char_loc in range(syn_start, char_idx+1):\n",
    "                    mapping[char_loc] = (acc, current_token_idx) # add to mapping\n",
    "                acc = '' # reset accumulator\n",
    "                current_token_idx += 1\n",
    "\n",
    "    if current_token_idx != len(context_tokens):\n",
    "        return None\n",
    "    else:\n",
    "        return mapping\n",
    "\n",
    "def write_to_file(out_file, line):\n",
    "    #out_file.write(line.encode('utf8') + '\\n'.encode('utf8'))\n",
    "    out_file.write(str(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_and_write(dataset, tier, out_dir):\n",
    "    \"\"\"Reads the dataset, extracts context, question, answer, tokenizes them,\n",
    "    and calculates answer span in terms of token indices.\n",
    "    Note: due to tokenization issues, and the fact that the original answer\n",
    "    spans are given in terms of characters, some examples are discarded because\n",
    "    we cannot get a clean span in terms of tokens.\n",
    "\n",
    "    This function produces the {train/dev}.{context/question/answer/span} files.\n",
    "\n",
    "    Inputs:\n",
    "      dataset: read from JSON\n",
    "      tier: string (\"train\" or \"dev\")\n",
    "      out_dir: directory to write the preprocessed files\n",
    "    Returns:\n",
    "      the number of (context, question, answer) triples written to file by the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    num_exs = 0 # number of examples written to file\n",
    "    num_mappingprob, num_tokenprob, num_spanalignprob, num_empty_charloc = 0, 0, 0,0\n",
    "    examples = []\n",
    "\n",
    "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
    "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "        for pid in range(len(article_paragraphs)):\n",
    "            context = str(article_paragraphs[pid]['context']) # string\n",
    "\n",
    "            # The following replacements are suggested in the paper\n",
    "            # BidAF (Seo et al., 2016)\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "\n",
    "            context_tokens = tokenize(context) # list of strings (lowercase)\n",
    "            context = context.lower()\n",
    "            \n",
    "            qas = article_paragraphs[pid]['qas']\n",
    "                \n",
    "            article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "\n",
    "            charloc2wordloc = get_char_word_loc_mapping(context, context_tokens) # charloc2wordloc maps the character location (int) of a context token to a pair giving (word (string), word loc (int)) of that token\n",
    "            '''\n",
    "            if charloc2wordloc is None: # there was a problem\n",
    "                num_mappingprob += len(qas)\n",
    "                continue # skip this context example\n",
    "            '''\n",
    "            # for each question, process the question and answer and write to file\n",
    "            for qn in qas:\n",
    "                \n",
    "                question = str(qn['question']) # string\n",
    "                question_tokens = tokenize(question) # list of strings\n",
    "\n",
    "                \n",
    "                #POSSIBLE QUESTIONS  \n",
    "                if qn['is_impossible']==False:\n",
    "                    impossible = 0\n",
    "                    artifical_answer = 0\n",
    "                    ans_text = str(qn['answers'][0]['text']).lower() \n",
    "                    ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
    "                    ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
    "\n",
    "                    # Check that the provided character spans match the provided answer text\n",
    "\n",
    "                    if str(context[ans_start_charloc:ans_end_charloc]) != str(ans_text):\n",
    "                        # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
    "                        num_spanalignprob += 1      \n",
    "                        continue\n",
    "\n",
    "                    # get word locs for answer start and end (inclusive)\n",
    "                    #if (articles_id==14):\n",
    "                        #print('pid=',pid)\n",
    "                        #print(charloc2wordloc)\n",
    "                    \n",
    "                    if charloc2wordloc is None:\n",
    "                        num_empty_charloc+=1\n",
    "                        continue\n",
    "                        \n",
    "                    ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "                    ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "                    assert ans_start_wordloc <= ans_end_wordloc\n",
    "\n",
    "                    # Check retrieved answer tokens match the provided answer text.\n",
    "                    # Sometimes they won't match, e.g. if the context contains the phrase \"fifth-generation\"\n",
    "                    # and the answer character span is around \"generation\",\n",
    "                    # but the tokenizer regards \"fifth-generation\" as a single token.\n",
    "                    # Then ans_tokens has \"fifth-generation\" but the ans_text is \"generation\", which doesn't match.\n",
    "                    ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
    "                    \n",
    "                    \n",
    "                    if str(context[ans_start_charloc:ans_end_charloc]) != str(ans_text):\n",
    "                        # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 \n",
    "                        #https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
    "                        num_spanalignprob += 1      \n",
    "                        continue\n",
    "                        \n",
    "                    ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
    "                    \n",
    "                    \n",
    "                    # get word locs for answer start and end (inclusive)\n",
    "                    ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "                    ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "                    assert ans_start_wordloc <= ans_end_wordloc\n",
    "                    \n",
    "                    \n",
    "                    if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
    "                        num_tokenprob += 1\n",
    "                        continue # skip this question/answer pair if \n",
    "                    \n",
    "                #IMPOSSIBLE QUESTIONS  \n",
    "                else: \n",
    "                    impossible = 1\n",
    "                    if len(qn['plausible_answers'])>0:\n",
    "                        artifical_answer = 1\n",
    "                        ans_text = str(qn['plausible_answers'][0]['text']).lower()    \n",
    "                        ans_start_charloc = qn['plausible_answers'][0]['answer_start'] # answer start loc (character count)\n",
    "                    \n",
    "                    else: #empty implausible questions, meaning questions with no artificial answers. \n",
    "                        artifical_answer = 0\n",
    "                        ans_text = 'N/A'\n",
    "                        ans_start_charloc = 'N/A'\n",
    "                        ans_end_charloc = 'N/A'\n",
    "                        ans_start_wordloc = 0.5\n",
    "                        ans_end_wordloc = 0.5\n",
    "                        num_mappingprob+=1\n",
    "\n",
    "                \n",
    "                \n",
    "                examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)]),' '.join(str(impossible)),' '.join(str(artifical_answer ))))\n",
    "\n",
    "                num_exs += 1\n",
    "\n",
    "    print(\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
    "    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
    "    print(\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n",
    "    print(\"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
    "    \n",
    "\n",
    "    indices = list(range(len(examples)))\n",
    "    #np.random.shuffle(indices)\n",
    "\n",
    "    with open(os.path.join(out_dir, tier +'.context'), 'w') as context_file,\\\n",
    "    open(os.path.join(out_dir, tier +'.question'), 'w') as question_file,\\\n",
    "    open(os.path.join(out_dir, tier +'.answer'), 'w') as ans_text_file, \\\n",
    "    open(os.path.join(out_dir, tier +'.span'), 'w') as span_file,\\\n",
    "    open(os.path.join(out_dir, tier +'.impossible'), 'w') as impossible_boolean_file,\\\n",
    "    open(os.path.join(out_dir, tier +'.artificial_answer'), 'w') as artificial_boolean_file:\n",
    "\n",
    "        for i in indices:\n",
    "            (context, question, answer, answer_span, impossible, artifical_answer) = examples[i]\n",
    "\n",
    "            # write tokenized data to file\n",
    "            write_to_file(context_file, context)\n",
    "            write_to_file(question_file, question)\n",
    "            write_to_file(ans_text_file, answer)\n",
    "            write_to_file(span_file, answer_span)\n",
    "            write_to_file(impossible_boolean_file, impossible)\n",
    "            write_to_file(artificial_boolean_file, artifical_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Now apply the Preprocessing onto Squad 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preprocessing train:   0%|          | 0/442 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data has 130319 examples total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing train: 100%|██████████| 442/442 [00:56<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
      "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  1158\n",
      "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  23\n",
      "Processed 129041 examples of total 130222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev:   3%|▎         | 1/35 [00:00<00:04,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev data has 11873 examples total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev: 100%|██████████| 35/35 [00:07<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  15\n",
      "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  74\n",
      "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  0\n",
      "Processed 11799 examples of total 11888\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## For anyone using this script, change the filename and data_dir as you please. \n",
    "train_filename = \"train-v2.0.json\"\n",
    "dev_filename = \"dev-v2.0.json\"\n",
    "data_dir = '/Users/kefei/Documents/DSML/NLP/Project_explore'\n",
    "\n",
    "## Organize training set\n",
    "train_data = data_from_json(train_filename)\n",
    "print(\"Train data has %i examples total\" % total_exs(train_data))\n",
    "preprocess_and_write(train_data, 'train', data_dir)\n",
    "\n",
    "## Organize dev set\n",
    "dev_data = data_from_json(dev_filename)\n",
    "print(\"Dev data has %i examples total\" % total_exs(dev_data))\n",
    "# preprocess dev set and write to file\n",
    "preprocess_and_write(dev_data, 'dev', data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Codes used to debug!\n",
    "\n",
    "\n",
    "dataset = dev_data\n",
    "articles_id=7\n",
    "article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "pid = 14\n",
    "\n",
    "context = str(article_paragraphs[pid]['context']) # string\n",
    "\n",
    "# The following replacements are suggested in the paper\n",
    "# BidAF (Seo et al., 2016)\n",
    "context = context.replace(\"''\", '\" ')\n",
    "context = context.replace(\"``\", '\" ')\n",
    "\n",
    "context_tokens = tokenize(context) # list of strings (lowercase)\n",
    "context = context.lower()\n",
    "\n",
    "qas = article_paragraphs[pid]['qas']\n",
    "qn = qas[0]\n",
    "question = str(qn['question']) # string\n",
    "question_tokens = tokenize(question) # list of strings\n",
    "ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
    "\n",
    "ans_text = str(qn['answers'][0]['text']).lower()\n",
    "ans_end_charloc = ans_start_charloc + len(ans_text) \n",
    "\n",
    "question = str(qn['question'])\n",
    "\n",
    "charloc2wordloc = get_char_word_loc_mapping(context, context_tokens)\n",
    "ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
